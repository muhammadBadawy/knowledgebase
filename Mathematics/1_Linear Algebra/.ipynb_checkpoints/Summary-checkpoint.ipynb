{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "\n",
    "1 - Building a matrix from linear equations\n",
    "\n",
    "2 - getting the size of a vector\n",
    "\n",
    "3 - finding the dot product\n",
    "\n",
    "4 - dot product is just the cosine rule (multiplication of lenth of vectors and cos the angle between them)\n",
    "\n",
    "5 - projetion (scalar projection of vector on the other is their dot product edvided by lenght of the other)\n",
    "\n",
    "6 - changing basis (if the new basis are orthogonal we can just find the projection of the desired vector on the new basis)\\\n",
    "    (if not orthogonal, then we use transformation matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 2&3\n",
    "\n",
    "1 - matrix multiplication with a vector (matrix transform a vector)\n",
    "    (we proved that a matrix affect the basis, and the vector follows the basis in this change)\n",
    "    \n",
    "2 - matrix transformation may be flip, inverse and rotate\n",
    "\n",
    "3 - determents of the matrix is the space (2D) or volume (3D) defined by the basis of the transformation matrix\n",
    "\n",
    "4 - matrix invers let's us undo tranformation of the matrix (we need determent to get the inverse, cuz if there is no determent, the matrix losses information while transformation and we can't undo the transformation)\n",
    "- (when lowering from 3D to 2D, volume becomes zero, cuz it turned to space (2D))\n",
    "- (when lowering from 2D to 1D, space becomes zero, cuz it turned to line (1D))\n",
    "\n",
    "5 - echlon form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 4\n",
    "\n",
    "1 - Einstein sum convention (how to multiply 2 matrices)\n",
    "\n",
    "2 - changing basis (sending vector to another basis then doing rotation or any transforamtion then getting the vector to our world again)\\\n",
    "    R(bears) = B.R(human).B^-1\n",
    "\n",
    "3 - Orthogonal matrix is where all vectors are othogonal (vec by another = 0), (in case of orthonormal, the inverse = the transpose)\n",
    "\n",
    "4 - we can do a lot of transformation and operations on vectors when they are orthogonal (we can make othogonal matrix from usual matrix using\n",
    "    Gram-schmidt process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# week 5\n",
    "\n",
    "1 - Eigenvectors of a matrix, are the vector that doesn't change their basis upon transformation\n",
    "\n",
    "2 - Calculating the Eigenvalues then the Eigenvectors\n",
    "\n",
    "3 - Diagonal matrix is a matrix that has values on it's main diagonal\n",
    "\n",
    "4 - When doing a transformation multiple times to a vector, we can say V of n = (T^n).(V of 0)\n",
    "\n",
    "5 - We can get T^n easily for diagonal matrices, so we want T to become diagonal\n",
    "\n",
    "6 - We can build a transformation matrix C from the Eigenvectors\n",
    "\n",
    "7 - We can diagonalize our transformation matrix T by the following D = C^-1 . T . C\n",
    "\n",
    "8 - Now we can get T^n by doing T^n = C . D^n . C^-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* In linear algebra why do I use equations as rows while solving system of linear equations but on the other hand I use the vectors as columns while doing row reductions to echlon form? Shouldn't the vector and the equations represent the same thing?\n",
    "\n",
    "https://www.quora.com/In-linear-algebra-why-do-I-use-equations-as-rows-while-solving-system-of-linear-equations-but-on-the-other-hand-I-use-the-vectors-as-columns-while-doing-row-reductions-to-echlon-form-Shouldnt-the-vector-and-the\n",
    "\n",
    "* Why coulms of transformation matrix are the unit vectors of the space defined by that matrix?\n",
    "\n",
    "https://www.quora.com/In-linear-algebra-I-noticed-that-the-columns-of-a-transformation-matrix-are-the-new-basis-of-the-transformed-vector-Why-and-how \\\n",
    "(This is a basic fact in linear algebra that you will find in any textbook or course; in fact it is really the way the transformation matrix is defined!,..... What you are asking about is based on the observation of the linear mapping)\n",
    "\n",
    "* Why when I multiply the transformation matrix by (inverse of the eigen matrix -defined by eigen vectors-) and then by the eigen\n",
    "   matrix it self, it becomes diagonalized? (note that some matrix transformation loses info -why not something similar happened-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
