{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to extended our ideas for a single variable in week 1 to a broader field, the multivariate case when we work with more than one variable at a time.\n",
    "\n",
    "Let'us start with the concept of \n",
    "# Dependent and independent variables.\n",
    "\n",
    "Think in the speed of a car:\n",
    "\n",
    "We can say that the speed of the car is a function of time, since at each point in time the car will be at a certain speed (like at 12:00 the car is at 103km/h, it is impossible for the car to have two different speeds at the exact time). But we can't say that time is a function of the speed of the car, since it is possible to have the same speed in different points of time. (12:00 is a 103km/h and at 13:00 is again at 103km/h).\n",
    "\n",
    "We say that the speed is a dependent variable, since it depends on time. And time is an independent variable, it doesn't depend on speed.\n",
    "\n",
    "\n",
    "Now that this is out of the way, let's learn to differentiate in a equation with more than one variable, and how this relates to constants.\n",
    "\n",
    "Given the following equation: $m = 2\\pi r^2t_p + 2\\pi rht_p$\n",
    "\n",
    "How we differentiate this function (m) in respect to a certain variable?\n",
    "\n",
    "You simple assume that all other variables are constant, except the one you are differentiating. (Remember the constant rule when differentiating)\n",
    "\n",
    "Let us derivate m in respect to h:\n",
    "\n",
    "$\\frac{\\partial m}{\\partial h} = 2\\pi rt_p$ This happens because since all other variables are constants, the term $2\\pi r^2t_p$ is a constant (for example equals 3) and it derivates to 0 since the variable h isn't attached to this term.\n",
    "\n",
    "\n",
    "So in short, we get all the patial derivatives and sum them to get the total derivative\n",
    "\n",
    "$\\partial f'(x ,y) = \\partial f_x '(x)+\\partial f_y '(y)$\n",
    " \n",
    "\n",
    "Now we are going to apply all together in a \n",
    "\n",
    "# Multivariate total derivative exemple\n",
    "\n",
    "Given the following function:\n",
    "\n",
    "$f(x,y,z) = sin(x)e^{yz^2}$\n",
    "\n",
    "Note that each variable (x,y,z) are dependent of another variable: t\n",
    "\n",
    "For instance: $x = t-1$ and $y=t^2$ and $z=1/t$\n",
    "\n",
    "We can think of this as a simple chain rule problem\n",
    "\n",
    "let us consider that the terms x, y and z are the unsubstitution of the variable t\n",
    "\n",
    "the different part here is that we have multi variables\n",
    "os let us start by finding the partial derivatives in respect to each term then sum them (cuz we are looking for total derivatives):\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = cos(x)e^{yz^2}$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial y} = $ Remember the chain rule? in this case would be:\n",
    "\n",
    "> $\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial u} \\frac{\\partial u}{\\partial y}$ And we call $e^{yz^2} = e^u$, with $u = yz^2$, so our expression reduces to: $sin(x)e^{u}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> so let us find $\\frac{\\partial f}{\\partial u} = sin(x)e^u$ and $\\frac{\\partial u}{\\partial y} = z^2$\n",
    "\n",
    "> Now finding the partial derivative: $\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial u} \\frac{\\partial u}{\\partial y} = sin(x)e^uz^2 = sin(x)e^{yz^2}2z^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial f}{\\partial z} =  sin(x)e^{yz^2}2yz$ (using the same logic as before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that each variable (x,y,z) are dependent of another variable: t\n",
    "\n",
    "$x = t-1$ and $y=t^2$ and $z=1/t$\n",
    "\n",
    "Now we use the chain rule to find the total derivative in respect to t:\n",
    "\n",
    "$\\frac{\\partial f(x,y,z)}{\\partial t} = \\frac{\\partial f}{\\partial x}\\frac{\\partial x}{\\partial t} + \\frac{\\partial f}{\\partial y}\\frac{\\partial y}{\\partial t} + \\frac{\\partial f}{\\partial z}\\frac{\\partial z}{\\partial t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the partial derivatives, we just need to find the derivative of the variable in respect to t:\n",
    "\n",
    "> $\\frac{\\partial x}{\\partial t} = 1$\n",
    "\n",
    "> $\\frac{\\partial y}{\\partial t} = 2t$\n",
    "\n",
    "> $\\frac{\\partial z}{\\partial t} = -1/t^2 = -t^{-2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we just to the multiplication of the chain rule to get:\n",
    "\n",
    "$\\frac{\\partial f(x,y,z)}{\\partial t} = cos(x)e^{yz^2}(1) + sin(x)e^{yz^2}2z^2(2t) + sin(x)e^{yz^2}2yz(-t^{-2})$\n",
    "\n",
    "If we substitute all x, y, and z for their expression in t, we will get a simplified final version of:\n",
    "\n",
    "$\\frac{\\partial f(x,y,z)}{\\partial t} = cos(t-1)e$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing partial differentiation\n",
    "\n",
    "# 2 -\n",
    "\n",
    "Given $f(x,y,z) = x^2y + y^2z + z^2x$, what are $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial y}$ and $\\frac{\\partial f}{\\partial z}$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\frac{\\partial f}{\\partial x} = 2xy + z^2$ (Note that the term $y^2z$ don't have x in it, so it is a constant and derivates to 0)\n",
    "\n",
    "> $\\frac{\\partial f}{\\partial y} = x^2 + 2yz$\n",
    "\n",
    "> $\\frac{\\partial f}{\\partial z} = y^2 + 2zx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 -\n",
    "\n",
    "Given $f(x,y,z) = e^{2x}\\sin(y)z^2 + \\cos(z)e^xe^y$, what are $\\frac{\\partial f}{\\partial x}$, $\\frac{\\partial f}{\\partial y} $ and $\\frac{\\partial f}{\\partial z}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial f}{\\partial x}$ for the first partial derivative:\n",
    "\n",
    "> Let us call $f(x,y,z) = g(x,y,z) + h(x,y,z)$ \n",
    "\n",
    "> $\\frac{\\partial f}{\\partial x} = \\frac{\\partial g}{\\partial x} + \\frac{\\partial h}{\\partial x}$\n",
    "\n",
    "> $\\frac{\\partial g}{\\partial x} = \\frac{\\partial g}{\\partial u}\\frac{\\partial u}{\\partial x}$\n",
    "\n",
    "> with $e^{2x} = e^u$ and $u = 2x$ our new expression for g is: $e^{u}\\sin(y)z^2$\n",
    "\n",
    "> $\\frac{\\partial g}{\\partial u} = e^u sin(y)z^2$\n",
    "\n",
    "> $\\frac{\\partial u}{\\partial x} = 2$\n",
    "\n",
    "> $\\frac{\\partial g}{\\partial x} = 2e^u sin(y)z^2 = 2e^{2x} sin(y)z^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> now for h:\n",
    "\n",
    "> $\\frac{\\partial h}{\\partial x} = cos(z)e^xe^y$\n",
    "\n",
    "> and finally:\n",
    "\n",
    "> $\\frac{\\partial f}{\\partial x} = \\frac{\\partial g}{\\partial x} + \\frac{\\partial h}{\\partial x} = 2e^{2x} sin(y)z^2 + cos(z)e^xe^y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial f}{\\partial y} = e^{2x}\\cos(y)z^2 + \\cos(z)e^xe^y$ \n",
    "\n",
    "$\\frac{\\partial f}{\\partial z} = 2e^{2x}\\cos(y)z - \\sin(z)e^xe^y$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobian\n",
    "\n",
    "The Jacobian is simply a vector that we can calculate for each location on a plot which points in the direction of the steepest uphill slope.\n",
    "\n",
    "(IMPORTANT NOTE: Note that every gradient (derivative) at any point has 2 properties which are direction (+\\-) and a magnitude (how steep the slope) towards that direction )\n",
    "\n",
    "Overlaying the Jacobian vector field, we can see that they are clearly all pointing uphill\n",
    "\n",
    "Furthermore, the steeper the slope, the greater the magnitude of Jacobian at that point. \n",
    "\n",
    "Whereas the peaks of the mountains and in the bottom of the valleys or even out on a wide flat plains, our gradients, and therefore our Jacobians are small. \n",
    "\n",
    "The jacobian is simple a vector containing the partial derivatives of a given function.\n",
    "\n",
    "so we can say at $f(x,y,z)$ our jacobian J is $[\\frac{\\delta f}{\\delta x}, \\frac{\\delta f}{\\delta y}, \\frac{\\delta f}{\\delta z}]$\n",
    "\n",
    "Noting that:\n",
    "\n",
    "$\\frac{\\delta f}{\\delta x}$ is the slope towards X\n",
    "\n",
    "$\\frac{\\delta f}{\\delta y}$ is the slope towards Y\n",
    "\n",
    "$\\frac{\\delta f}{\\delta z}$ is the slope towards Z\n",
    "\n",
    "Note that before the jacobians when we was doing the total derivative, we used to sum the previous derivatives to get the total, and now we want the derivative for each direction alone so we mande each one separete in a vector\n",
    "\n",
    "So the Jacobian is just an algebraic expression for a vector which when we give it a specific\n",
    "x, y, z coordinate, will return\n",
    "a vector pointing in the direction of steepest slope of this function. \n",
    "\n",
    "We note that the direction of the jacobian is depending on the direction of its values\n",
    "\n",
    "![jacobians_direction](images/jacobians_direction.png)\n",
    "\n",
    "Note that when we were dealing with single variable calculus we split the region into many many many small lines and calculated the gradient for them, but now we are dealing with multivariate, so we are dealing with areas and spaces not lengths, so we are calculating the derivative of a area or space not a tiny lines.\n",
    "\n",
    "This is pretty obvious from our graph, as we notice the space is divided into tiny squares\n",
    "\n",
    "If we have a function: $f(x,y) = xy$, our partial derivatives would be: $y$ and $x$, so our jacobian would be the vector:\n",
    "> $J = (y,x)$\n",
    "\n",
    "This is a vector that will point in the direction of the steepest slope of the function when given some coordinate.\n",
    "\n",
    "This means that when we calculate a jacobian with a random x and y, it will point to the points of maximas and minimas of the function. BUT be careful, a given function can have multiple points of maxima/minima!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# practice - calculating the jacobian\n",
    "\n",
    "# 1 -\n",
    "\n",
    "For $f(x,y) = x^2y + \\frac{3}{4}xy + 10$, calculate the Jacobian row vector J.\n",
    "\n",
    ">$\\frac{\\partial f}{\\partial x} = 2xy + \\frac{3}{4}y$\n",
    "\n",
    ">$\\frac{\\partial f}{\\partial y} = x^2 + \\frac{3}{4}x$\n",
    "\n",
    "> $J = [xy + \\frac{3}{4}y,x^2 + \\frac{3}{4}x]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 -\n",
    "For $f(x,y,z) = x^2 + 3e^ye^z + cos(x)sin(z)$,calculate the the Jacobian row vector and evaluate at the point (0, 0, 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$\\frac{\\partial f}{\\partial x} = 2x - sin(x)sin(z)$\n",
    "\n",
    ">$\\frac{\\partial f}{\\partial y} = 3e^ye^z$\n",
    "\n",
    ">$\\frac{\\partial f}{\\partial z} = 3e^ye^z + cos(x)cos(z)$\n",
    "\n",
    "$J = [2x - sin(x)sin(z),3e^ye^z,3e^ye^z + cos(x)cos(z)]$\n",
    "\n",
    "At point (0,0,0) \n",
    "\n",
    "$J = [2.0 - sin(0)sin(0),3e^0e^0,3e^0e^0 + cos(0)cos(0)] = [0,3.1.1,3.1.1 + 1.1] = [0,3,4]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobian matrix\n",
    "\n",
    "a Jacobian matrix which describes functions that take a vector as an input, but unlike our previous examples, also give a vector as the output.\n",
    "\n",
    "If we consider the two functions\n",
    "\n",
    "\n",
    "![constructing jacobian matrix 1](images/constructing_jacobian_matrix_1.png)\n",
    "\n",
    "\n",
    "we can think of these as two vector spaces (Remember changin basis in Linear Algebra)\n",
    "one containing vectors with coordinates in u v and the other with coordinates in x and y.\n",
    "Each point in x y has a corresponding location in u v. So,\n",
    "as we move around x y space,\n",
    "we would expect our corresponding path in u v space to be quite different and so it is.\n",
    "\n",
    "We can of course, make separate row vector Jacobians for u and v. However, as we are considering u and v to be components of a single vector, it makes more sense to extend our Jacobian by stacking these vectors as rows of a matrix like this.\n",
    "\n",
    "![constructing jacobian matrix 2](images/constructing_jacobian_matrix_2.png)\n",
    "\n",
    "Now let's calculate the matrix\n",
    "\n",
    "![constructing jacobian matrix 3](images/constructing_jacobian_matrix_3.png)\n",
    "\n",
    "Our Jacobian matrix no longer even contains any variables, which is what we should expect when we consider that clearly, both u and v are linear functions of x and y.\n",
    "\n",
    "So the gradient must be constant everywhere.\n",
    "\n",
    "Also, this matrix is just the linear transformation from xy space to uv space.\n",
    "\n",
    "Note : In linear algebra all of our functions was \"linear\" and we was just taking the cooficients cuz they were the derivative of a linear function.\n",
    "\n",
    "---\n",
    "of course, many of the functions that you'll be confronted with, will be highly nonlinear, and generally much more complicated than the simple linear example we've just looked at here.\n",
    "\n",
    "However, often they may still be smooth, which means that if we zoom in close enough, we can consider each little region of space (it's space here cuz it's nonlinear -it was length before because it was linear-) to be approximately linear\n",
    "\n",
    "Note that when we were dealing with single variable calculus we split the region into many many many small lines and calculated the gradient for them, but now we are dealing with multivariate, so we are dealing with areas and spaces not lengths, so we are calculating the derivative of a area or space not a tiny lines.\n",
    "\n",
    "This is pretty obvious from our graph, as we notice the space is divided into tiny squares\n",
    "\n",
    "![jacobians_on_area](images/jacobians_on_area.png)\n",
    "\n",
    "We note that the direction of the jacobian is depending on the direction of its values\n",
    "\n",
    "And therefore, by adding up all the contributions from the Jacobian determinants (cuz the determent of the jacobian is the change of area while transforming from x,y space to u,v space - read https://www.coursera.org/learn/multivariate-calculus-machine-learning/discussions/weeks/2/threads/5BSUiqM8EemIrhJW1joUvA -) at each point in space, we can still calculate the change in the size of a region after transformation.\n",
    "\n",
    "But never forget that when we were dealing with the univariate system we could get the contribution of the jacobian by getting the determinant of $[df/dx]$ whis is 1×1 system, but now we are dealing with the multivariate system we need to get the determined of 2×2 and 3×3 matrix\n",
    "\n",
    "A classic example of this occurs when transforming between\n",
    "cartesian and polar coordinate systems.\n",
    "So, if we have a vector expressed in terms of a radius r,\n",
    "and the angle up from the x-axis is theta,\n",
    "but we'd like them expressed in terms of x and y instead.\n",
    "\n",
    "We can write the following expressions just by thinking about trigonometry.\n",
    "Now, we can build the Jacobian matrix and take its determinant.\n",
    "\n",
    "![constructing jacobian matrix 4](images/constructing_jacobian_matrix_4.png)\n",
    "\n",
    "The fact that the result is simply the radius r,\n",
    "and not the function theta,\n",
    "tells us that as we move along r (r here is a dimension, not just a constant),\n",
    "away from the origin,\n",
    "small regions of space will scale as a function of r.\n",
    "\n",
    "![constructing jacobian matrix 5](images/constructing_jacobian_matrix_5.png)\n",
    "\n",
    "So the jacobian matrix means $\\delta u \\delta v = r.(\\delta x \\delta y)$\n",
    "\n",
    "This means if we take area $\\delta x \\delta y$ at x, y space and scale it by $r$ it gives area $\\delta u \\delta v$ at new u, v space\n",
    "\n",
    "Interactive Transcript - Enable basic transcript mode by pressing the escape key\n",
    "\n",
    "We've now seen that the Jacobian describes the gradient of a multivariable system.\n",
    "And if you calculate it for a scalar valued multivariable function,\n",
    "you get a row vector pointing up the direction of greater slope,\n",
    "with a length proportional to the local steepness (local here means the local tiny square we are calculating it's gradient). \n",
    "\n",
    "And we are going to use this to is the Oprimization\n",
    "\n",
    "we use the word optimisation to describe\n",
    "the process of trying to make something as good as it can be.\n",
    "In mathematics it is the dedicated research to find the input values to functions,\n",
    "which correspond to either a maximum or a minimum of a system.\n",
    "\n",
    "we can solve such system analytically by first building the Jacobian, \n",
    "And then finding the values of x and y which make it equal to 0\n",
    "\n",
    "\n",
    "However, when the function gets a bit more complicated, finding the maximum or\n",
    "minimum can get a bit tricky.\n",
    "\n",
    "If we still have an analytical expression, then we can at least still find the general expression for the Jacobian.\n",
    "\n",
    "But now simply setting it to 0 is not only much more complicated but it also is not enough as this function has multiple locations with zero gradient.\n",
    "\n",
    "For example:\n",
    "\n",
    "![gradients_haet_map](images/gradients_haet_map.png)\n",
    "\n",
    "If we assume that all of the maxima and minima of this function can be seen\n",
    "in the region we're plotting here, then just looking at the surface plot of\n",
    "our function makes it very clear where the tallest peak and the deepest trough are.\n",
    "\n",
    "We refer to all the peaks as maxima.\n",
    "\n",
    "But in this case, we have a single tallest peak A,\n",
    "which we will call the global maximum\n",
    "\n",
    "as well as several local maxima at C and E.\n",
    "\n",
    "Similarly, we refer to all the troughs as minima and\n",
    "we also have a single deepest point at D which we call the global minimum,\n",
    "as well as a local minimum at point B.\n",
    "\n",
    "\n",
    "Imagine standing on the surface with its hills and valleys, and\n",
    "we're trying to climb to the top of the highest peak.\n",
    "That's no problem, we just look around, spot the tallest mountain and\n",
    "walk straight towards it.\n",
    "But what if we're walking at night?\n",
    "This would be much like the scenario where we didn't have a nice\n",
    "analytical expression for our function.\n",
    "\n",
    "so we are going to solve it using a torch (jacobians),\n",
    "we can see the Jacobian vectors painted on road signs all around us and\n",
    "each one would say, peak - this way.\n",
    "\n",
    "![jacobians_as_torch](images/jacobians_as_torch.png)\n",
    "\n",
    "We would have to remember that although the Jacobians all point uphill,\n",
    "they don't necessarily point to the top of the tallest hill.\n",
    "And you could find yourself walking up to one of the local maxima at C or E.\n",
    "And even worse, when you get there,\n",
    "you'll find that all of the road signs are pointing directly at you.\n",
    "\n",
    "It's no problem to effectively transport all over the map by teleportation (you don't have to walk step by step).\n",
    "\n",
    "As you can try the function at many different places, but there's no need to evaluate everywhere in between.\n",
    "And the calculation costs the same at any point as we only substituting in the jacobian, essentially,\n",
    "no matter how far apart the points are, so we're not really walking, we are just teleporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hessian\n",
    "\n",
    "The hessian is a extension of the jacobian. In the hessian we will make a matrix (not a vector) with the second order derivatives.\n",
    "\n",
    "To calculate the secord order derivative, we first calculate the first derivative, then calculate the derivative of this first derivative.\n",
    "\n",
    "Similarly for a partial derivative, if you want to find the second derivative with\n",
    "respect to x1 then x2, it's as simple as just differentiating with respect to x1,\n",
    "assuming all the other variables are constant, and then differentiating with\n",
    "respect to x2, again, assuming all the other variables are constant.\n",
    "\n",
    "As you can see from this general form, our Hessian matrix will be\n",
    "an n by n square matrix, where n is the number of variables in our function f. \n",
    "\n",
    "To calculate the Hessian\n",
    "\n",
    "It's like finding the jacobian of a function but put it in column vector (not row vector), then find the jacobian of every elemenent in the earlier jacobian vector\n",
    "\n",
    "The notation is as follows:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x}$ first order derivative of f in relation to x\n",
    "$\\frac{\\partial^2 f}{\\partial x}$ second order derivative of f in relation to x\n",
    "\n",
    "So the hessian matrix will have the following form:\n",
    "\n",
    "$ H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1x_2} & ... & \\frac{\\partial^2 f}{\\partial x_1x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & ... & \\frac{\\partial^2 f}{\\partial x_2x_n} \\\\ . & . & ... & . \\\\ \\frac{\\partial^2 f}{\\partial x_nx_1} & \\frac{\\partial^2 f}{\\partial x_nx_2} & ... & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{bmatrix}$\n",
    "\n",
    "\n",
    "Quick example\n",
    "\n",
    ">$f(x,y,z) = x^2yz$\n",
    "\n",
    "> $ J = [2xyz,x^2z,x^2y]$ (first order derivatives)\n",
    "\n",
    "> $H = \\begin{bmatrix} 2yz & 2xz & 2xy \\\\ 2xz & 0 & x^2 \\\\ 2xy & x^2 & 0 \\end{bmatrix}$\n",
    "\n",
    "* Note that the hessian is simmetryc\n",
    "\n",
    "So one thing to notice here is that our Hessian matrix is symmetrical\n",
    "across the leading diagonal.\n",
    "So actually, once I'd worked out the top right region,\n",
    "I could just have written these directly in for the bottom left region.\n",
    "\n",
    "This will always be true if the function is continuous,\n",
    "meaning that it has no sudden step changes.\n",
    "\n",
    "(Remember the non continuous $f(x) = \\frac {1}{x}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hessian gives us information if our jacobian is dealling with a maxima or a minima (or neither!)\n",
    "\n",
    "If the determinant of the Hessian is positive: Our jacobian is pointing towards a maxima or a minima.\n",
    "\n",
    "- If the first term of the hessian is positive: we are pointing towards a minima, and if is negative it is a maxima\n",
    "\n",
    "If the determinant is negative we are dealing with a saddle point (this can confuse the search of the maxima/minima points!)\n",
    "\n",
    "\n",
    "for example\n",
    "\n",
    "for this function:\n",
    "\n",
    "![3d_function_1_1](images/3d_function_1_1.png)\n",
    "\n",
    "![3d_function](images/3d_function.png)\n",
    "\n",
    "If you hadn't known what function we were dealing with and\n",
    "calculated the value of the Jacobian at the point (0,0)\n",
    "you'd have seen that the gradient vector was also 0.\n",
    "\n",
    "To know if it is maximum or a minimum at that point?\n",
    "\n",
    "You could, of course, go and check some other point and see if it was above or\n",
    "below, but this isn't very robust.\n",
    "\n",
    "Instead, we can look at the Hessian, which in this simple case is no longer even a function of x or y.\n",
    "\n",
    "Its determinant is 4 (positive).\n",
    "\n",
    "Then look at the first term,\n",
    "which is sitting at the top left-hand corner of the Hessian.\n",
    "\n",
    "If it is positive, we know we've got a minimum\n",
    "\n",
    "If it is negative, we've got a maximum.\n",
    "\n",
    "Another example:\n",
    "\n",
    "![3d_function_2_1](images/3d_function_2_1.png)\n",
    "\n",
    "![3d_function_2](images/3d_function_2.png)\n",
    "\n",
    "This time, our Hessian determinant is negative.\n",
    "\n",
    "So we know that we're not dealing with a maximum or a minimum.\n",
    "\n",
    "But clearly at this point, (0,0), the gradient is flat.\n",
    "\n",
    "Well, if you look at the function graph,\n",
    "\n",
    "what we've got here is a location with 0 gradient, but with slopes coming down towards it in one direction, but up towards it in the other.\n",
    "\n",
    "We call this kind of feature as saddle point (if det of Hessian is negative), and\n",
    "they can also cause a lot of confusion when searching for a peak.\n",
    "\n",
    "At last module, we're also going to see another way that the Hessian can help us with optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice hessians\n",
    "\n",
    "# 1 -\n",
    "\n",
    "For the function $f(x,y) = x^3y + x + 2y$, calculate the Hessian matrix $H = \\begin{bmatrix} \\partial_{x,x}f & \\partial_{x,y}f \\\\ \\partial_{y,x}f & \\partial_{y,y}f \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\partial_{x}f = 3x^2y+1$\n",
    "\n",
    ">> $\\partial_{x,x}f = 6xy$\n",
    "\n",
    ">> $\\partial_{x,y}f = 3x^2$\n",
    "\n",
    "> $\\partial_{y}f = x^3+2$\n",
    "\n",
    ">> $\\partial_{y,x}f = 3x^2$\n",
    "\n",
    ">> $\\partial_{y,y}f = 0$"
   ]
  },
  {
   "attachments": {
    "158d3459-e628-4cc6-9158-a22ebc515b30.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hessian_exer](images/hessian_exer.png)\n",
    "\n",
    "We notice that the function returns the same\n",
    "\n",
    "![image.png](attachment:158d3459-e628-4cc6-9158-a22ebc515b30.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
